{
    "metadata": {
        "kernelspec": {
            "name": "sparkkernel",
            "display_name": "Spark | Scala"
        },
        "language_info": {
            "name": "scala",
            "mimetype": "text/x-scala",
            "codemirror_mode": "text/x-scala",
            "pygments_lexer": "scala"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<p align=\"center\">\r\n",
                "<img src =\"https://raw.githubusercontent.com/microsoft/azuredatastudio/master/src/sql/media/microsoft_logo_gray.svg?sanitize=true\" width=\"250\" align=\"center\">\r\n",
                "</p>\r\n",
                "\r\n",
                "# **Spark Package Management in SQL Server 2019 Big Data Clusters**\r\n",
                "This guide covers installing packages and submitting jobs to a SQL Server 2019 Big Data Cluster using Spark.\r\n",
                "* Built-In Tools\r\n",
                "* Install Packages from a Maven Repository onto the Spark Cluster at Runtime\r\n",
                "* Import .jar from HDFS for use at runtime\r\n",
                "* Import .jar at runtime through Azure Data Studio notebook cell configuration\r\n",
                "* Install Python Packages at Runtime for use with PySpark \r\n",
                "* Submit local .jar or python file\r\n",
                "<!-- <span style=\"color:red\"><font size=\"3\">Please press the \"Run Cells\" button to run the notebook</font></span> -->\r\n",
                "\r\n",
                "For more information on package managament, refer to [Spark library management](https://docs.microsoft.com/sql/big-data-cluster/spark-install-packages?view=sql-server-ver15)"
            ],
            "metadata": {
                "azdata_cell_guid": "cbc8ced8-8931-4302-b252-7e7e478a16d4"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Built-in Tools\r\n",
                "* Spark and Hadoop base packages\r\n",
                "* Python 3.8 with PySpark and Pandas, Sklearn, Numpy, and other data processing libraries.\r\n",
                "* R 3.5 with Spark.R, sparklyr and MRO packages\r\n"
            ],
            "metadata": {
                "azdata_cell_guid": "2fc8a069-115e-4d9b-bedc-5c55f79466b1"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Install Packages from a Maven Repository onto the Spark Cluster at Runtime\r\n",
                "Maven packages can be installed onto your Spark cluster using notebook cell configuration at the start of your spark session. Before starting a spark session in Azure Data Studio, run the following code:\r\n",
                "\r\n",
                "```\r\n",
                "%%configure -f \\\r\n",
                "{\"conf\": {\"spark.jars.packages\": \"com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.1\"}}\r\n",
                "```\r\n"
            ],
            "metadata": {
                "azdata_cell_guid": "a0fecc05-f094-4dda-9afe-0de8ddad87eb"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Import .jar from HDFS for use at runtime\n",
                "\n",
                "Import jar at runtime through Azure Data Studio notebook cell configuration.\n",
                "\n",
                "```\n",
                "%%configure -f\n",
                "{\"conf\": {\"spark.jars\": \"/jar/mycodeJar.jar\"}}\n",
                "```\n"
            ],
            "metadata": {
                "azdata_cell_guid": "c5e65fa2-faf0-4e22-aac1-69d7ff8c9989"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Import .jar at runtime through Azure Data Studio notebook cell configuration\n",
                "\n",
                "```\n",
                "%%configure -f\n",
                "{\"conf\": {\"spark.jars\": \"/jar/mycodeJar.jar\"}}\n",
                "```\n"
            ],
            "metadata": {
                "azdata_cell_guid": "6fc4085f-e142-4355-b215-148dbf6c5b86"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Install Python Packages at Runtime for use with PySpark\r\n",
                "\r\n",
                "This capability changed significantly after SQL Server Big Data Clusters CU10.\r\n",
                "\r\n",
                "For more information on this scenario, refer to [Spark library management](https://docs.microsoft.com/sql/big-data-cluster/spark-install-packages?view=sql-server-ver15)\r\n"
            ],
            "metadata": {
                "azdata_cell_guid": "07944b55-7266-4fcd-8e9b-9fd6cb8cfef5"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Submit local .jar or python file\r\n",
                "One of the key scenarios for big data clusters is the ability to submit Spark jobs for SQL Server. The Spark job submission feature allows you to submit a local Jar or Py files with references to SQL Server 2019 big data cluster. It also enables you to execute a Jar or Py files, which are already located in the HDFS file system.\r\n",
                "\r\n",
                "* [Submit Spark jobs on SQL Server Big Data Clusters in Azure Data Studio](https://docs.microsoft.com/en-us/sql/big-data-cluster/spark-submit-job?view=sqlallproducts-allversions)\r\n",
                "* [Submit Spark jobs on SQL Server Big Data Clusters in IntelliJ](https://docs.microsoft.com/en-us/sql/big-data-cluster/spark-submit-job-intellij-tool-plugin?view=sqlallproducts-allversions)\r\n",
                "* [Submit Spark jobs on SQL Server big data cluster in Visual Studio Code](https://docs.microsoft.com/en-us/sql/big-data-cluster/spark-hive-tools-vscode?view=sqlallproducts-allversions)\r\n"
            ],
            "metadata": {
                "azdata_cell_guid": "7d1b55c0-1961-45f7-8449-a24a913106e4"
            }
        }
    ]
}